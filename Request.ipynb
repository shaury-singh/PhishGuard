{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "[59, 12, 0, 2, 3, 0, 0, 0, 0, 0, 0, 1, 0, 5, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0.06779661016949153, 0.0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 8, 3, 3, 2, False, 0, 0, 0, False, 0, 0, 0, False, 1, 0.0, 1.0, False, 1, 0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse, urljoin\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import tldextract\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.slant.co/topics/2404/~file-managers-for-windows\"\n",
    "\n",
    "def length_of_hostname(url):\n",
    "    length = len(urlparse(url).hostname)\n",
    "    return length\n",
    "\n",
    "def length_of_ip(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    hostname = parsed_url.hostname\n",
    "    ip_pattern = r\"^\\d{1,3}(\\.\\d{1,3}){3}$\"\n",
    "    if hostname and re.match(ip_pattern, hostname):\n",
    "        return len(hostname)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def http_in_path(url):\n",
    "    path = urlparse(url).path\n",
    "    return 1 if \"http\" in path else 0\n",
    "\n",
    "def https_token(url):\n",
    "    path = urlparse(url).path\n",
    "    return 1 if \"https\" in path else 0\n",
    "\n",
    "def ratio_digits_url(url):\n",
    "    return sum(c.isdigit() for c in url) / len(url)\n",
    "\n",
    "def ratio_digits_host(url):\n",
    "    hostname = urlparse(url).hostname\n",
    "    return sum(c.isdigit() for c in hostname) / len(hostname) if hostname else 0\n",
    "\n",
    "def punycode(url):\n",
    "    hostname = urlparse(url).hostname\n",
    "    return 1 if hostname and \"xn--\" in hostname else 0\n",
    "\n",
    "def get_extension_length(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    path = parsed_url.path\n",
    "    _, ext = os.path.splitext(path)\n",
    "    return len(ext)\n",
    "\n",
    "def nb_redirection(url):\n",
    "    return url.count(\"//\") - 1\n",
    "\n",
    "def get_nb_external_redirections(url):\n",
    "    try:\n",
    "        session = requests.Session()\n",
    "        response = session.get(url, allow_redirects=True)\n",
    "        original_domain = urlparse(url).netloc\n",
    "        nb_external_redirections = 0\n",
    "        for resp in response.history:\n",
    "            redirected_domain = urlparse(resp.url).netloc\n",
    "            if redirected_domain != original_domain:\n",
    "                nb_external_redirections += 1\n",
    "                original_domain = redirected_domain  \n",
    "        return nb_external_redirections\n",
    "    except requests.exceptions.RequestException:\n",
    "        return 0\n",
    "\n",
    "def get_length_words_raw(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    raw_text = parsed_url.netloc + parsed_url.path + parsed_url.query\n",
    "    words = re.split(r'[\\W_]+', raw_text)\n",
    "    return len([word for word in words if word])-1\n",
    "\n",
    "def check_port_flag(url):\n",
    "    parsed = urlparse(url)\n",
    "    return 1 if parsed.port else 0\n",
    "\n",
    "def tld_in_path(url):\n",
    "    tld = tldextract.extract(url).suffix\n",
    "    return 1 if tld in urlparse(url).path else 0\n",
    "\n",
    "def tld_in_subdomain(url):\n",
    "    extracted = tldextract.extract(url)\n",
    "    return 1 if extracted.suffix in extracted.subdomain else 0\n",
    "\n",
    "def abnormal_subdomain(url):\n",
    "    return 1 if urlparse(url).hostname.count('.') > 2 else 0\n",
    "\n",
    "def nb_subdomains(url):\n",
    "    return urlparse(url).hostname.count('.')\n",
    "\n",
    "def prefix_suffix(url):\n",
    "    return 1 if \"-\" in urlparse(url).hostname else 0\n",
    "\n",
    "def random_domain(url):\n",
    "    hostname = urlparse(url).hostname or \"\"\n",
    "    return 1 if re.match(r'^[a-zA-Z0-9]{7,}$', hostname.split('.')[0]) else 0\n",
    "\n",
    "def shortening_service(url):\n",
    "    shorteners = [\"bit.ly\", \"t.co\", \"goo.gl\", \"tinyurl.com\", \"is.gd\", \"ow.ly\"]\n",
    "    return 1 if any(service in url for service in shorteners) else 0\n",
    "\n",
    "def get_char_repeat(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    raw_text = parsed_url.netloc + parsed_url.path + parsed_url.query\n",
    "    max_repeat = max((len(m.group()) for m in re.finditer(r'(.)\\1*', raw_text)), default=1)\n",
    "    return max_repeat\n",
    "\n",
    "def get_shortest_words_raw(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    raw_text = parsed_url.netloc + parsed_url.path + parsed_url.query\n",
    "    words = re.split(r'[\\W_]+', raw_text)\n",
    "    shortest_word_length = min((len(word) for word in words if word), default=0)\n",
    "    return shortest_word_length+1\n",
    "\n",
    "def get_shortest_word_host(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    host = parsed_url.netloc\n",
    "    words = re.split(r'[.-]', host)\n",
    "    shortest_word_length = min((len(word) for word in words if word), default=0)\n",
    "    return shortest_word_length\n",
    "\n",
    "def extract_url_features(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    host = parsed_url.netloc\n",
    "    path = parsed_url.path\n",
    "    raw_text = host + path + parsed_url.query\n",
    "    words_raw = re.split(r'[\\W_]+', raw_text)  \n",
    "    words_host = re.split(r'[.-]', host)       \n",
    "    words_path = re.split(r'[\\W_]+', path)     \n",
    "    shortest_word_path = min((len(word) for word in words_path if word), default=0)\n",
    "    longest_words_raw = max((len(word) for word in words_raw if word), default=0)\n",
    "    longest_word_host = max((len(word) for word in words_host if word), default=0)\n",
    "    longest_word_path = max((len(word) for word in words_path if word), default=0)\n",
    "    return shortest_word_path, longest_words_raw, longest_word_host, longest_word_path\n",
    "\n",
    "PHISHING_KEYWORDS = [\"secure\", \"account\", \"login\", \"bank\", \"verify\", \"password\", \"update\", \"confirm\", \"webscr\", \"signin\", \"ebayisapi\", \"paypal\", \"click\", \"auth\", \"identity\"]\n",
    "def extract_url_features(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    host = parsed_url.netloc\n",
    "    path = parsed_url.path\n",
    "    raw_text = host + path + parsed_url.query\n",
    "    words_raw = re.split(r'[\\W_]+', raw_text)  \n",
    "    words_host = re.split(r'[.-]', host)       \n",
    "    words_path = re.split(r'[\\W_]+', path)     \n",
    "    avg_words_raw = sum(len(word) for word in words_raw if word) / len(words_raw) if words_raw else 0\n",
    "    avg_word_host = sum(len(word) for word in words_host if word) / len(words_host) if words_host else 0\n",
    "    avg_word_path = sum(len(word) for word in words_path if word) / len(words_path) if words_path else 0\n",
    "    phish_hints = sum(1 for word in words_raw if word.lower() in PHISHING_KEYWORDS)\n",
    "    return avg_words_raw, avg_word_host, avg_word_path, phish_hints\n",
    "\n",
    "BRAND_NAMES = [\"paypal\", \"google\", \"facebook\", \"amazon\", \"bank\", \"microsoft\", \"apple\", \"netflix\", \"ebay\"]\n",
    "SUSPICIOUS_TLDS = [\"tk\", \"ml\", \"ga\", \"cf\", \"gq\", \"xyz\", \"top\", \"biz\", \"club\", \"pw\", \"info\"]\n",
    "\n",
    "def extract_url_features(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc\n",
    "    path = parsed_url.path\n",
    "    parts = domain.split(\".\")\n",
    "    main_domain = parts[-2] if len(parts) >= 2 else \"\"\n",
    "    subdomain = \".\".join(parts[:-2]) if len(parts) > 2 else \"\"\n",
    "    domain_in_brand = any(brand in main_domain.lower() for brand in BRAND_NAMES)\n",
    "    brand_in_subdomain = any(brand in subdomain.lower() for brand in BRAND_NAMES)\n",
    "    brand_in_path = any(brand in path.lower() for brand in BRAND_NAMES)\n",
    "    tld = parts[-1] if len(parts) > 1 else \"\"\n",
    "    suspecious_tld = tld in SUSPICIOUS_TLDS\n",
    "    return domain_in_brand, brand_in_subdomain, brand_in_path, suspecious_tld\n",
    "\n",
    "BLACKLISTED_DOMAINS = {\"phishingsite.com\", \"malicious-login.tk\", \"scam-bank.xyz\"}\n",
    "def extract_url_features(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc\n",
    "    statistical_report = domain in BLACKLISTED_DOMAINS\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        all_links = [a.get(\"href\") for a in soup.find_all(\"a\", href=True)]\n",
    "        nb_hyperlinks = len(all_links)\n",
    "        internal_links = []\n",
    "        external_links = []\n",
    "        for link in all_links:\n",
    "            absolute_link = urljoin(url, link) \n",
    "            link_domain = urlparse(absolute_link).netloc\n",
    "            if link_domain == domain:\n",
    "                internal_links.append(link)\n",
    "            else:\n",
    "                external_links.append(link)\n",
    "        ratio_intHyperlinks = len(internal_links) / nb_hyperlinks if nb_hyperlinks else 0\n",
    "        ratio_extHyperlinks = len(external_links) / nb_hyperlinks if nb_hyperlinks else 0\n",
    "    except requests.RequestException:\n",
    "        nb_hyperlinks = 0\n",
    "        ratio_intHyperlinks = 0\n",
    "        ratio_extHyperlinks = 0\n",
    "    return statistical_report, nb_hyperlinks, ratio_intHyperlinks, ratio_extHyperlinks\n",
    "\n",
    "def URL_Feature_check(url):\n",
    "    parameter = []\n",
    "    parameter.append(len(url))\n",
    "    parameter.append(length_of_hostname(url))\n",
    "    parameter.append(length_of_ip(url))\n",
    "    parameter.append(url.count('.')) #dots\n",
    "    parameter.append(url.count('-')) #hyphens\n",
    "    parameter.append(url.count('@')) #@\n",
    "    parameter.append(url.count('?')) #?\n",
    "    parameter.append(url.count('&')) #&\n",
    "    parameter.append(url.count('|')) #pipeline\n",
    "    parameter.append(url.count('=')) #eq\n",
    "    parameter.append(url.count('_')) #underscore\n",
    "    parameter.append(url.count('~')) #tilde\n",
    "    parameter.append(url.count('%')) #percentage\n",
    "    parameter.append(url.count('/')) #slash\n",
    "    parameter.append(url.count('*')) #star \n",
    "    parameter.append(url.count(':'))    \n",
    "    parameter.append(url.count(','))\n",
    "    parameter.append(url.count(';'))\n",
    "    parameter.append(url.count('$'))\n",
    "    parameter.append(url.count(' '))\n",
    "    parameter.append(url.count('www'))\n",
    "    parameter.append(url.count('com'))\n",
    "    parameter.append(url.count('//'))\n",
    "    parameter.append(http_in_path(url))\n",
    "    parameter.append(https_token(url))\n",
    "    parameter.append(ratio_digits_url(url))\n",
    "    parameter.append(ratio_digits_host(url))\n",
    "    parameter.append(punycode(url))\n",
    "    parameter.append(check_port_flag(url)) \n",
    "    parameter.append(tld_in_path(url))\n",
    "    parameter.append(tld_in_subdomain(url))\n",
    "    parameter.append(abnormal_subdomain(url))\n",
    "    parameter.append(nb_subdomains(url))\n",
    "    parameter.append(prefix_suffix(url))\n",
    "    parameter.append(random_domain(url))\n",
    "    parameter.append(shortening_service(url))\n",
    "    parameter.append(get_extension_length(url)) # --- path extension\n",
    "    parameter.append(nb_redirection(url))\n",
    "    parameter.append(get_nb_external_redirections(url))\n",
    "    parameter.append(get_length_words_raw(url)) \n",
    "    parameter.append(get_char_repeat(url))\n",
    "    parameter.append(get_shortest_words_raw(url))\n",
    "    parameter.append(get_shortest_word_host(url))\n",
    "    shortest_word_path, longest_words_raw, longest_word_host, longest_word_path = extract_url_features(url)\n",
    "    parameter.append(shortest_word_path)\n",
    "    parameter.append(longest_words_raw)\n",
    "    parameter.append(longest_word_host)\n",
    "    parameter.append(longest_word_path)\n",
    "    avg_words_raw, avg_word_host, avg_word_path, phish_hints = extract_url_features(url)\n",
    "    parameter.append(avg_words_raw)\n",
    "    parameter.append(avg_word_host)\n",
    "    parameter.append(avg_word_path)\n",
    "    parameter.append(phish_hints)\n",
    "    domain_in_brand, brand_in_subdomain, brand_in_path, suspecious_tld = extract_url_features(url)\n",
    "    parameter.append(domain_in_brand)\n",
    "    parameter.append(brand_in_subdomain)\n",
    "    parameter.append(brand_in_path)\n",
    "    parameter.append(suspecious_tld)\n",
    "    statistical_report, nb_hyperlinks, ratio_intHyperlinks, ratio_extHyperlinks = extract_url_features(url)\n",
    "    parameter.append(statistical_report)\n",
    "    parameter.append(nb_hyperlinks)\n",
    "    parameter.append(ratio_intHyperlinks)\n",
    "    parameter.append(ratio_extHyperlinks)\n",
    "    print(len(parameter))\n",
    "    return parameter\n",
    "\n",
    "print(URL_Feature_check(url))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
