{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing started!\n",
      "[59, 12, 14, 2, 3, 0, 0, 0, 0, 0, 0, 1, 0, 5, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0.06779661016949153, 0.0, 0, 443, 0, 0, 0, 2, 0, 0, 1, 0, 0, 59, 5, 2, 5, 3.3333333333333335, 2, 5, 3.3333333333333335, 2, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 7284, 2788, 1, 7, 2]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import whois\n",
    "import requests\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import socket\n",
    "import tldextract\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def random_domain(url):\n",
    "    hostname = urlparse(url).hostname or \"\"\n",
    "    return 1 if re.match(r'^[a-zA-Z0-9]{7,}$', hostname.split('.')[0]) else 0\n",
    "\n",
    "def shortening_service(url):\n",
    "    shorteners = [\"bit.ly\", \"t.co\", \"goo.gl\", \"tinyurl.com\", \"is.gd\", \"ow.ly\"]\n",
    "    return 1 if any(service in url for service in shorteners) else 0\n",
    "\n",
    "def path_extension(url):\n",
    "    path = urlparse(url).path\n",
    "    return path.split(\".\")[-1] if \".\" in path else \"\"\n",
    "\n",
    "def nb_redirection(url):\n",
    "    return url.count(\"//\") - 1\n",
    "\n",
    "def length_words_raw(url):\n",
    "    return len(url)\n",
    "\n",
    "\n",
    "def char_repeat(url):\n",
    "    return max(Counter(url).values())\n",
    "\n",
    "def word_lengths(hostname):\n",
    "    words = hostname.replace(\"-\", \" \").split(\".\")\n",
    "    if not words:\n",
    "        return 0, 0, 0\n",
    "    return min(map(len, words)), max(map(len, words)), sum(map(len, words)) / len(words)\n",
    "\n",
    "def extract_word_features(url):\n",
    "    hostname = urlparse(url).hostname or \"\"\n",
    "    return word_lengths(hostname)\n",
    "\n",
    "def phish_hints(url):\n",
    "    suspicious_words = [\"secure\", \"account\", \"bank\", \"login\", \"verify\", \"update\"]\n",
    "    return 1 if any(word in url.lower() for word in suspicious_words) else 0\n",
    "\n",
    "known_brands = [\"paypal\", \"google\", \"facebook\", \"bank\"]\n",
    "\n",
    "def brand_in_domain(url):\n",
    "    domain = tldextract.extract(url).domain\n",
    "    return 1 if any(brand in domain.lower() for brand in known_brands) else 0\n",
    "\n",
    "def brand_in_subdomain(url):\n",
    "    subdomain = tldextract.extract(url).subdomain\n",
    "    return 1 if any(brand in subdomain.lower() for brand in known_brands) else 0\n",
    "\n",
    "def brand_in_path(url):\n",
    "    path = urlparse(url).path\n",
    "    return 1 if any(brand in path.lower() for brand in known_brands) else 0\n",
    "\n",
    "suspicious_tlds = {\"xyz\", \"top\", \"tk\", \"ml\", \"cf\", \"ga\"}\n",
    "def get_tld(url):\n",
    "    extracted = tldextract.extract(url)\n",
    "    return extracted.suffix\n",
    "\n",
    "def is_suspicious_tld(url):\n",
    "    tld = tldextract.extract(url).suffix\n",
    "    return 1 if tld in suspicious_tlds else 0\n",
    "\n",
    "\n",
    "def domain_registration_length(url):\n",
    "    try:\n",
    "        domain_info = whois.whois(urlparse(url).hostname)\n",
    "        expiration_date = domain_info.expiration_date\n",
    "        creation_date = domain_info.creation_date\n",
    "        if isinstance(expiration_date, list):\n",
    "            expiration_date = expiration_date[0]\n",
    "        if isinstance(creation_date, list):\n",
    "            creation_date = creation_date[0]\n",
    "        return (expiration_date - creation_date).days if expiration_date and creation_date else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def domain_age(url):\n",
    "    try:\n",
    "        domain_info = whois.whois(urlparse(url).hostname)\n",
    "        creation_date = domain_info.creation_date\n",
    "        if isinstance(creation_date, list):\n",
    "            creation_date = creation_date[0]\n",
    "        return (datetime.now() - creation_date).days if creation_date else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def web_traffic(url):\n",
    "    try:\n",
    "        domain = urlparse(url).hostname\n",
    "        alexa_url = f\"https://data.alexa.com/data?cli=10&dat=s&url={domain}\"\n",
    "        response = requests.get(alexa_url)\n",
    "        if \"<POPULARITY URL=\" in response.text:\n",
    "            rank = response.text.split(\"<POPULARITY URL=\")[1].split(\"TEXT=\")[1].split(\"/>\")[0]\n",
    "            return int(rank)\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def google_index(url):\n",
    "    try:\n",
    "        query = f\"site:{urlparse(url).hostname}\"\n",
    "        response = requests.get(f\"https://www.google.com/search?q={query}\")\n",
    "        return 1 if \"did not match any documents\" not in response.text else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def page_rank(url):\n",
    "    try:\n",
    "        rank_api = f\"https://api.example.com/pagerank?domain={urlparse(url).hostname}\"\n",
    "        response = requests.get(rank_api)\n",
    "        return int(response.text) if response.status_code == 200 else 0\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def http_in_path(url):\n",
    "    path = urlparse(url).path\n",
    "    return 1 if \"http\" in path else 0\n",
    "\n",
    "def https_token(url):\n",
    "    path = urlparse(url).path\n",
    "    return 1 if \"https\" in path else 0\n",
    "\n",
    "def ratio_digits_url(url):\n",
    "    return sum(c.isdigit() for c in url) / len(url)\n",
    "\n",
    "def ratio_digits_host(url):\n",
    "    hostname = urlparse(url).hostname\n",
    "    return sum(c.isdigit() for c in hostname) / len(hostname) if hostname else 0\n",
    "\n",
    "def punycode(url):\n",
    "    hostname = urlparse(url).hostname\n",
    "    return 1 if hostname and \"xn--\" in hostname else 0\n",
    "\n",
    "def port(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    return parsed_url.port if parsed_url.port else (-1 if parsed_url.scheme == \"http\" else 443)\n",
    "\n",
    "def tld_in_path(url):\n",
    "    tld = tldextract.extract(url).suffix\n",
    "    return 1 if tld in urlparse(url).path else 0\n",
    "\n",
    "def tld_in_subdomain(url):\n",
    "    extracted = tldextract.extract(url)\n",
    "    return 1 if extracted.suffix in extracted.subdomain else 0\n",
    "\n",
    "def abnormal_subdomain(url):\n",
    "    return 1 if urlparse(url).hostname.count('.') > 2 else 0\n",
    "\n",
    "def nb_subdomains(url):\n",
    "    return urlparse(url).hostname.count('.')\n",
    "\n",
    "def prefix_suffix(url):\n",
    "    return 1 if \"-\" in urlparse(url).hostname else 0\n",
    "\n",
    "\n",
    "def get_nb_hyperlinks(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return 0\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        links = soup.find_all(\"a\")\n",
    "        return len(links)\n",
    "    except Exception as e:\n",
    "        return 0\n",
    "\n",
    "def get_ratio_intHyperlinks(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return 0  \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        all_links = soup.find_all(\"a\")  \n",
    "        total_links = len(all_links)\n",
    "        if total_links == 0:\n",
    "            return 0  \n",
    "        domain = urlparse(url).netloc  \n",
    "        internal_links = 0\n",
    "        for link in all_links:\n",
    "            href = link.get(\"href\")\n",
    "            if href:\n",
    "                full_url = urljoin(url, href)  \n",
    "                if urlparse(full_url).netloc == domain:  \n",
    "                    internal_links += 1\n",
    "        ratio = internal_links / total_links  \n",
    "        return round(ratio, 4)  \n",
    "    except Exception as e:\n",
    "        return 0\n",
    "    \n",
    "def get_ratio_extHyperlinks(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return 0  \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        all_links = soup.find_all(\"a\")  \n",
    "        total_links = len(all_links)\n",
    "        if total_links == 0:\n",
    "            return 0  \n",
    "        domain = urlparse(url).netloc  \n",
    "        external_links = 0\n",
    "        for link in all_links:\n",
    "            href = link.get(\"href\")\n",
    "            if href:\n",
    "                full_url = urljoin(url, href)  \n",
    "                if urlparse(full_url).netloc and urlparse(full_url).netloc != domain:  \n",
    "                    external_links += 1\n",
    "        ratio = external_links / total_links  \n",
    "        return round(ratio, 4)  \n",
    "    except Exception as e:\n",
    "        return 0\n",
    "\n",
    "def get_ratio_nullHyperlinks(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return 0  \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        all_links = soup.find_all(\"a\")  \n",
    "        total_links = len(all_links)\n",
    "        if total_links == 0:\n",
    "            return 0  \n",
    "        null_links = sum(1 for link in all_links if not link.get(\"href\") or link.get(\"href\").strip() == \"\")\n",
    "        ratio = null_links / total_links  \n",
    "        return round(ratio, 4)  \n",
    "    except Exception as e:\n",
    "        return 0  \n",
    "\n",
    "def get_nb_extCSS(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return 0  \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        css_links = soup.find_all(\"link\", rel=\"stylesheet\")  \n",
    "        domain = urlparse(url).netloc\n",
    "        external_css = 0\n",
    "        for link in css_links:\n",
    "            href = link.get(\"href\")\n",
    "            if href:\n",
    "                full_url = urljoin(url, href)  \n",
    "                if urlparse(full_url).netloc and urlparse(full_url).netloc != domain:  \n",
    "                    external_css += 1\n",
    "        return external_css  \n",
    "    except Exception as e:\n",
    "        return 0  \n",
    "\n",
    "def get_ratio_intRedirection(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return 0  \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        all_links = soup.find_all(\"a\", href=True)  \n",
    "        total_redirections = 0\n",
    "        internal_redirections = 0\n",
    "        domain = urlparse(url).netloc\n",
    "        for link in all_links:\n",
    "            href = link.get(\"href\")\n",
    "            full_url = urljoin(url, href)  \n",
    "            try:\n",
    "                redir_response = requests.head(full_url, allow_redirects=True, timeout=3)\n",
    "                if redir_response.url != full_url:  \n",
    "                    total_redirections += 1\n",
    "                    if urlparse(redir_response.url).netloc == domain:  \n",
    "                        internal_redirections += 1\n",
    "            except requests.RequestException:\n",
    "                continue  \n",
    "        if total_redirections == 0:\n",
    "            return 0  \n",
    "        ratio = internal_redirections / total_redirections  \n",
    "        return round(ratio, 4)  \n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def get_ratio_extRedirection(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return 0  \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        all_links = soup.find_all(\"a\", href=True)  \n",
    "        total_redirections = 0\n",
    "        external_redirections = 0\n",
    "        domain = urlparse(url).netloc\n",
    "        for link in all_links:\n",
    "            href = link.get(\"href\")\n",
    "            full_url = urljoin(url, href)  \n",
    "            try:\n",
    "                redir_response = requests.head(full_url, allow_redirects=True, timeout=3)\n",
    "                if redir_response.url != full_url:  \n",
    "                    total_redirections += 1\n",
    "                    if urlparse(redir_response.url).netloc != domain:\n",
    "                        external_redirections += 1\n",
    "            except requests.RequestException:\n",
    "                continue  \n",
    "        if total_redirections == 0:\n",
    "            return 0  \n",
    "        ratio = external_redirections / total_redirections  \n",
    "        return round(ratio, 4)  \n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def get_ratio_intErrors(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return 0  \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        all_links = soup.find_all(\"a\", href=True)  \n",
    "        domain = urlparse(url).netloc\n",
    "        total_internal_links = 0\n",
    "        internal_error_links = 0\n",
    "        for link in all_links:\n",
    "            href = link.get(\"href\")\n",
    "            full_url = urljoin(url, href)  \n",
    "            if urlparse(full_url).netloc == domain:  \n",
    "                total_internal_links += 1\n",
    "                try:\n",
    "                    link_response = requests.head(full_url, timeout=3)\n",
    "                    if link_response.status_code >= 400:  \n",
    "                        internal_error_links += 1\n",
    "                except requests.RequestException:\n",
    "                    internal_error_links += 1  \n",
    "        if total_internal_links == 0:\n",
    "            return 0  \n",
    "        ratio = internal_error_links / total_internal_links  \n",
    "        return round(ratio, 4)  \n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def get_ratio_extErrors(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return 0  \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        all_links = soup.find_all(\"a\", href=True)  \n",
    "        domain = urlparse(url).netloc\n",
    "        total_external_links = 0\n",
    "        external_error_links = 0\n",
    "        for link in all_links:\n",
    "            href = link.get(\"href\")\n",
    "            full_url = urljoin(url, href)  \n",
    "            if urlparse(full_url).netloc != domain:  \n",
    "                total_external_links += 1\n",
    "                try:\n",
    "                    link_response = requests.head(full_url, timeout=3)  \n",
    "                    if link_response.status_code >= 400:  \n",
    "                        external_error_links += 1\n",
    "                except requests.RequestException:\n",
    "                    external_error_links += 1  \n",
    "        if total_external_links == 0:\n",
    "            return 0  \n",
    "        ratio = external_error_links / total_external_links  \n",
    "        return round(ratio, 4)  \n",
    "    except Exception:\n",
    "        return 0 \n",
    "\n",
    "def has_login_form(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return 0  \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        forms = soup.find_all(\"form\")  \n",
    "        login_keywords = [\"login\", \"signin\", \"log-in\", \"sign-in\", \"password\"]\n",
    "        for form in forms:\n",
    "            inputs = form.find_all(\"input\")\n",
    "            has_password = any(inp.get(\"type\") == \"password\" for inp in inputs)\n",
    "            has_login_keyword = any(\n",
    "                any(keyword in (form.get(attr) or \"\").lower() for keyword in login_keywords)\n",
    "                for attr in [\"id\", \"name\", \"class\", \"action\"]\n",
    "            )\n",
    "            if has_password or has_login_keyword:\n",
    "                return 1  \n",
    "        return 0  \n",
    "    except Exception:\n",
    "        return 0 \n",
    "\n",
    "def get_links_in_tags(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return 0  \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        head_links = soup.head.find_all(\"a\", href=True) if soup.head else []\n",
    "        meta_links = soup.find_all(\"meta\", attrs={\"content\": True})\n",
    "        script_links = soup.find_all(\"script\", attrs={\"src\": True})\n",
    "        meta_href_count = sum(1 for meta in meta_links if \"http\" in meta.get(\"content\", \"\"))\n",
    "        script_href_count = sum(1 for script in script_links if \"http\" in script.get(\"src\", \"\"))\n",
    "        total_links = len(head_links) + meta_href_count + script_href_count\n",
    "        return total_links\n",
    "    except Exception:\n",
    "        return 0 \n",
    "\n",
    "def has_submit_email(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return 0  \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        forms = soup.find_all(\"form\")  \n",
    "        email_keywords = [\"email\", \"mail\"]\n",
    "        for form in forms:\n",
    "            inputs = form.find_all(\"input\")\n",
    "            has_email_input = any(inp.get(\"type\") == \"email\" for inp in inputs)\n",
    "            has_email_keyword = any(\n",
    "                any(keyword in (form.get(attr) or \"\").lower() for keyword in email_keywords)\n",
    "                for attr in [\"id\", \"name\", \"class\", \"action\"]\n",
    "            )\n",
    "            if has_email_input or has_email_keyword:\n",
    "                return 1  \n",
    "        return 0  \n",
    "    except Exception:\n",
    "        return 0 \n",
    "\n",
    "def get_ratio_intMedia(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return 0  \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        domain = urlparse(url).netloc  \n",
    "        media_tags = soup.find_all([\"img\", \"video\", \"audio\", \"source\"])  \n",
    "        total_media = len(media_tags)\n",
    "        if total_media == 0:\n",
    "            return 0  \n",
    "        internal_media = sum(1 for tag in media_tags if tag.get(\"src\") and urlparse(tag[\"src\"]).netloc in [\"\", domain])\n",
    "        return internal_media / total_media  \n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def get_ratio_extMedia(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return 0  \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        domain = urlparse(url).netloc  \n",
    "        media_tags = soup.find_all([\"img\", \"video\", \"audio\", \"source\"])  \n",
    "        total_media = len(media_tags)\n",
    "        if total_media == 0:\n",
    "            return 0  \n",
    "        external_media = sum(1 for tag in media_tags if tag.get(\"src\") and urlparse(tag[\"src\"]).netloc not in [\"\", domain])\n",
    "        return external_media / total_media  \n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def get_sfh(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return 0  \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        domain = urlparse(url).netloc  \n",
    "        forms = soup.find_all(\"form\")  \n",
    "        if not forms:\n",
    "            return 1  \n",
    "        for form in forms:\n",
    "            action = form.get(\"action\", \"\").strip() \n",
    "            if action in [\"\", \"#\"]:\n",
    "                return -1  \n",
    "            action_domain = urlparse(action).netloc\n",
    "            if action_domain == \"\" or action_domain == domain:\n",
    "                return 1 \n",
    "            return 0\n",
    "        return 1  \n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def get_iframe_ratio(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return 0  # Return 0 if the page is not accessible\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        total_elements = len(soup.find_all())  # Total HTML elements\n",
    "        iframe_count = len(soup.find_all(\"iframe\"))  # Count of iframe elements\n",
    "\n",
    "        return iframe_count / total_elements if total_elements else 0  # Ratio of iframes\n",
    "    except Exception:\n",
    "        return 0 \n",
    "    \n",
    "def has_popup_window(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return 0  # Page not accessible\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        scripts = soup.find_all(\"script\")\n",
    "\n",
    "        for script in scripts:\n",
    "            if script.string and re.search(r\"window\\.open|window\\.showModalDialog\", script.string):\n",
    "                return 1  # Suspicious popup detected\n",
    "\n",
    "        return 0  # No suspicious popups found\n",
    "    except Exception:\n",
    "        return 0 \n",
    "    \n",
    "def get_safe_anchor_ratio(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return 0  # Return 0 if the page is not accessible\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        anchors = soup.find_all(\"a\")\n",
    "        total_anchors = len(anchors)\n",
    "\n",
    "        if total_anchors == 0:\n",
    "            return 1  # If no anchors, assume safe\n",
    "\n",
    "        safe_anchors = sum(1 for a in anchors if a.get(\"href\") and not a[\"href\"].startswith((\"#\", \"javascript:void(0)\")))\n",
    "\n",
    "        return safe_anchors / total_anchors  # Ratio of safe anchors\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def is_right_click_disabled(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return 0  # Page not accessible\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Check for oncontextmenu attribute in body\n",
    "        if soup.body and \"oncontextmenu\" in soup.body.attrs:\n",
    "            if soup.body[\"oncontextmenu\"].strip().lower() == \"return false;\":\n",
    "                return 1  # Right-click is disabled\n",
    "\n",
    "        # Check for JavaScript disabling right-click\n",
    "        scripts = soup.find_all(\"script\")\n",
    "        for script in scripts:\n",
    "            if script.string and re.search(r\"event\\.button\\s*==\\s*2|oncontextmenu\\s*=\\s*['\\\"]return false;['\\\"]\", script.string):\n",
    "                return 1  # Suspicious\n",
    "\n",
    "        return 0  # Right-click is allowed\n",
    "    except Exception:\n",
    "        return 0 \n",
    "    \n",
    "def has_empty_title(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return 1  # Consider empty if page not accessible\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        title = soup.title.string.strip() if soup.title else \"\"\n",
    "\n",
    "        return 1 if not title else 0  # 1 if empty, 0 if title exists\n",
    "    except Exception:\n",
    "        return 1 \n",
    "    \n",
    "def is_domain_in_title(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return 0  # Consider it safe if page is inaccessible\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        title = soup.title.string.lower().strip() if soup.title else \"\"\n",
    "        domain = urlparse(url).netloc.split(\".\")[-2]  # Extract main domain name\n",
    "\n",
    "        return 1 if domain in title else 0  # 1 if domain is in title, otherwise 0\n",
    "    except Exception:\n",
    "        return 0\n",
    "    \n",
    "def is_domain_in_copyright(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code != 200:\n",
    "            return 0  # Consider safe if page is inaccessible\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        domain = urlparse(url).netloc.split(\".\")[-2]  # Extract main domain name\n",
    "        copyright_patterns = [\"©\", \"copyright\", \"all rights reserved\"]\n",
    "        \n",
    "        text = soup.get_text().lower()\n",
    "        if any(phrase in text for phrase in copyright_patterns):\n",
    "            return 1 if domain in text else 0  # 1 if domain is mentioned, otherwise 0\n",
    "\n",
    "        return 0  # No copyright mention\n",
    "    except Exception:\n",
    "        return 0\n",
    "    \n",
    "def get_registered_domain(url):\n",
    "    try:\n",
    "        domain_info = whois.whois(url)\n",
    "        return len(domain_info.domain_name)  # Returns the registered domain name\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def get_domain_registration_length(url):\n",
    "    try:\n",
    "        domain_info = whois.whois(url)\n",
    "        if isinstance(domain_info.expiration_date, list):\n",
    "            expiration_date = domain_info.expiration_date[0]\n",
    "        else:\n",
    "            expiration_date = domain_info.expiration_date\n",
    "\n",
    "        if isinstance(domain_info.creation_date, list):\n",
    "            creation_date = domain_info.creation_date[0]\n",
    "        else:\n",
    "            creation_date = domain_info.creation_date\n",
    "\n",
    "        if expiration_date and creation_date:\n",
    "            return (expiration_date - creation_date).days  # Registration length in days\n",
    "        return 0\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def get_domain_registration_length(url):\n",
    "    try:\n",
    "        domain_info = whois.whois(url)\n",
    "        if isinstance(domain_info.expiration_date, list):\n",
    "            expiration_date = domain_info.expiration_date[0]\n",
    "        else:\n",
    "            expiration_date = domain_info.expiration_date\n",
    "\n",
    "        if isinstance(domain_info.creation_date, list):\n",
    "            creation_date = domain_info.creation_date[0]\n",
    "        else:\n",
    "            creation_date = domain_info.creation_date\n",
    "\n",
    "        if expiration_date and creation_date:\n",
    "            return (expiration_date - creation_date).days  # Registration length in days\n",
    "        return 0\n",
    "    except Exception:\n",
    "        return 0\n",
    "    \n",
    "def get_domain_age(url):\n",
    "    try:\n",
    "        domain_info = whois.whois(url)\n",
    "        if isinstance(domain_info.creation_date, list):\n",
    "            creation_date = domain_info.creation_date[0]\n",
    "        else:\n",
    "            creation_date = domain_info.creation_date\n",
    "\n",
    "        if creation_date:\n",
    "            return (datetime.now() - creation_date).days  # Domain age in days\n",
    "        return 0\n",
    "    except Exception:\n",
    "        return 0\n",
    "    \n",
    "def get_alexa_rank(url):\n",
    "    try:\n",
    "        domain = url.replace(\"http://\", \"\").replace(\"https://\", \"\").split(\"/\")[0]  # Extract domain name\n",
    "        response = requests.get(f\"https://data.alexa.com/data?cli=10&dat=snbamz&url={domain}\")\n",
    "        \n",
    "        if \"<POPULARITY\" in response.text:\n",
    "            rank = response.text.split('TEXT=\"')[1].split('\"')[0]\n",
    "            return int(rank)\n",
    "        return 0  # If no ranking found\n",
    "    except Exception:\n",
    "        return 0\n",
    "# [51, 17, 13(0), 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0.0, 0.0, 0, 443, 0, 0, 0, 2, 0, 0, 0, 0, 0, 51, 5, 3, 9, 5.0, 3, 9, 5.0, 3, 9, 0, 0, 0, 0, 0, 262, 0.9389, 0.0496, 0.0038, 3, 0.9412, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 7284, 2788, 1, 7, 2]\n",
    "def url_preprocessing(url):\n",
    "    parameters = []\n",
    "    print(\"preprocessing started!\")\n",
    "    parameters.append(len(url)) #url\n",
    "    parameters.append(len(urlparse(url).hostname)) #hostname\n",
    "    parameters.append(len(socket.gethostbyname(urlparse(url).hostname))) #ip -- wrong\n",
    "    parameters.append(url.count('.')) #dots\n",
    "    parameters.append(url.count('-')) #hyphens\n",
    "    parameters.append(url.count('@')) #@\n",
    "    parameters.append(url.count('?')) #?\n",
    "    parameters.append(url.count('&')) #&\n",
    "    parameters.append(url.count('|')) #pipeline\n",
    "    parameters.append(url.count('=')) #eq\n",
    "    parameters.append(url.count('_')) #underscore\n",
    "    parameters.append(url.count('~')) #tilde\n",
    "    parameters.append(url.count('%')) #percentage\n",
    "    parameters.append(url.count('/')) #slash\n",
    "    parameters.append(url.count('*')) #star\n",
    "    parameters.append(url.count(':'))    \n",
    "    parameters.append(url.count(','))\n",
    "    parameters.append(url.count(';'))\n",
    "    parameters.append(url.count('$'))\n",
    "    parameters.append(url.count(' '))\n",
    "    parameters.append(url.count('www'))\n",
    "    parameters.append(url.count('com'))\n",
    "    parameters.append(url.count('//'))   \n",
    "    parameters.append(http_in_path(url))\n",
    "    parameters.append(https_token(url))\n",
    "    parameters.append(ratio_digits_url(url))\n",
    "    parameters.append(ratio_digits_host(url))\n",
    "    parameters.append(punycode(url))\n",
    "    parameters.append(port(url))\n",
    "    parameters.append(tld_in_path(url))\n",
    "    parameters.append(tld_in_subdomain(url))\n",
    "    parameters.append(abnormal_subdomain(url))\n",
    "    parameters.append(nb_subdomains(url))\n",
    "    parameters.append(prefix_suffix(url))\n",
    "    parameters.append(random_domain(url))\n",
    "    parameters.append(shortening_service(url))\n",
    "    # parameters.append(path_extension(url))\n",
    "    parameters.append(nb_redirection(url))\n",
    "    parameters.append(nb_redirection(url))\n",
    "    parameters.append(length_words_raw(url))\n",
    "    parameters.append(char_repeat(url))\n",
    "    shortest_word_host, longest_word_host, avg_word_host = extract_word_features(url)\n",
    "    parameters.append(shortest_word_host)\n",
    "    parameters.append(longest_word_host)\n",
    "    parameters.append(avg_word_host)\n",
    "    parameters.append(shortest_word_host)\n",
    "    parameters.append(longest_word_host)\n",
    "    parameters.append(avg_word_host)\n",
    "    parameters.append(shortest_word_host)\n",
    "    parameters.append(longest_word_host)\n",
    "    # check once again\n",
    "    parameters.append(phish_hints(url))\n",
    "    parameters.append(brand_in_domain(url))\n",
    "    parameters.append(brand_in_subdomain(url))\n",
    "    parameters.append(brand_in_path(url))\n",
    "    parameters.append(is_suspicious_tld(url))\n",
    "    parameters.append(get_nb_hyperlinks(url))\n",
    "    parameters.append(get_ratio_intHyperlinks(url))\n",
    "    parameters.append(get_ratio_extHyperlinks(url))\n",
    "    parameters.append(get_ratio_nullHyperlinks(url))\n",
    "    parameters.append(get_nb_extCSS(url))\n",
    "    parameters.append(get_ratio_intRedirection(url))\n",
    "    parameters.append(get_ratio_extRedirection(url))\n",
    "    parameters.append(get_ratio_extErrors(url))\n",
    "    parameters.append(has_login_form(url))\n",
    "    parameters.append(get_links_in_tags(url))\n",
    "    parameters.append(has_submit_email(url))\n",
    "    parameters.append(get_ratio_extMedia(url))\n",
    "    parameters.append(get_sfh(url))\n",
    "    parameters.append(get_iframe_ratio(url))\n",
    "    parameters.append(has_popup_window(url))\n",
    "    parameters.append(get_safe_anchor_ratio(url))\n",
    "    parameters.append(has_empty_title(url))\n",
    "    parameters.append(has_empty_title(url))\n",
    "    parameters.append(is_domain_in_title(url))\n",
    "    parameters.append(is_domain_in_copyright(url))\n",
    "    parameters.append(get_registered_domain(url))\n",
    "    parameters.append(get_domain_registration_length(url))\n",
    "    parameters.append(get_domain_age(url))\n",
    "    parameters.append(get_alexa_rank(url))\n",
    "    parameters.append(0)\n",
    "    parameters.append(0)\n",
    "    parameters.append(1)\n",
    "    parameters.append(0)\n",
    "    parameters.append(0)\n",
    "    parameters.append(7284)\n",
    "    parameters.append(2788)\n",
    "    parameters.append(1)\n",
    "    parameters.append(7)\n",
    "    parameters.append(2)\n",
    "    print(parameters)\n",
    "    return parameters\n",
    "\n",
    "#print(url_preprocessing(\"https://console.cloud.google.com/apis/dashboard\"))\n",
    "parameters = url_preprocessing(\"https://www.slant.co/topics/2404/~file-managers-for-windows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phishing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaur\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but RandomForestClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "rf_model = joblib.load('phishing_detector.pkl')\n",
    "# parameters = [51,17,0,3,0,0,0,0,0,0,0,0,0,4,0,1,0,0,0,0,1,0,0,0,0,0.0,0.0,0,0,0,0,0,3,0,0,0,0,2,0,5,3,3,3,4,12,9,12,7.0,6.0,7.666666667,0,0,0,0,0,0,147,0.7959183670000001,0.204081633,0,3,0,0.133333333,0,0.0,0,1,0.0,0,81.25,18.75,0,0,0,16.66666667,0,0,0,0,1,0,1847,7284,2788,0,1,7]\n",
    "parameters = np.array(parameters).reshape(1, -1)\n",
    "y_pred = rf_model.predict(parameters)\n",
    "print(\"Phishing\" if y_pred[0] == 1 else \"Legitimate\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
